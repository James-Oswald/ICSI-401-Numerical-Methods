
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[normalem]{ulem}
\usepackage{listings}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{array}
\usepackage{float}
\restylefloat{table}
%fancy headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\lhead{ICSI 401 Homework 4}
\rhead{\thepage}
\author{James Oswald}
\date{November 18, 2020}
\title{ICSI 401 Homework 4}

\begin{document}
\maketitle
\thispagestyle{fancy}
\addtocounter{section}{4}
\subsection{Partial pivoting}
In this problem, you will review partial pivoting and the reasoning behind it.
\begin{itemize}
    \item[1.]  Circle the best explanation for the use of partial pivoting from the choices below.
    \begin{itemize}
        \item[(a)] Partial pivoting is used in Gaussian elimination to mitigate the effects of rounding errors in the matrix entries, resulting in an algorithm that is usually backward stable.
        \item[(b)] Partial pivoting is not used in Gaussian elimination at all, but is instead used to solve linear systems via Cramer’s rule.
        \item[(c)] Partial pivoting is used to transform the input to Gaussian elimination so as to speed up the solution of linear systems.
    \end{itemize}
    \noindent
    \newline\newline\newline
    The correct answer is choice A: Partial pivoting is used in Gaussian elimination to mitigate the effects of rounding errors in the matrix entries, resulting in an algorithm that is usually backward stable.
    
    \newpage
    \item[2.] . Show the steps of Gaussian elimination with partial pivoting for the following matrix:
    \[\begin{pmatrix} 10^{-5} & 1 & 0 \\ 2 & -3 & 1 \\ -1 & 1 & 1 \end{pmatrix}\]
    Make sure that you clearly indicate in each step what operation you are performing (e.g., which rows are swapped, which multiples of rows are added to which other rows) and the result of that operation. You should end up with an upper triangular matrix.
    \begin{align*}
        \begin{pmatrix} 0.00001 & 1 & 0 \\ 2 & -3 & 1 \\ -1 & 1 & 1 \end{pmatrix}
        & R_2\leftrightarrow R_1 & \text{Pivot element is 2}\\
        \begin{pmatrix} 2 & -3 & 1 \\ 0.00001 & 1 & 0 \\ -1 & 1 & 1 \end{pmatrix}
        & \frac{1}{2}R_1\rightarrow R_1 &\\
        \begin{pmatrix} 1 & -1.5 & 0.5 \\ 0.00001 & 1 & 0 \\ -1 & 1 & 1 \end{pmatrix}
        & R_2 + (-0.0001)R_1\rightarrow R_2 &\\
        \begin{pmatrix} 1 & -1.5 & 0.5 \\ 0 & 1.000015 & 1.000005 \\ -1 & 1 & 1 \end{pmatrix}
        & R_3 + R_1\rightarrow R_3 &\\
        \begin{pmatrix} 1 & -1.5 & 0.5 \\ 0 & 1.000015 & 1.000005 \\ 0 & -0.5 & 1.5 \end{pmatrix}
        & \frac{1}{1.000015}R_2\rightarrow R_2 & \text{Pivot element is 1.000015} \\
        \begin{pmatrix} 1 & -1.5 & 0.5 \\ 0 & 1 & 0.000004999 \\ 0 & -0.5 & 1.5 \end{pmatrix}
        & R_3 + (0.5)R_2\rightarrow R_3 & \\
        \begin{pmatrix} 1 & -1.5 & 0.5 \\ 0 & 1 & 0.000004999 \\ 0 & 0 & 1.500002499 \end{pmatrix}
        & \frac{1}{1.500002499}R_3\rightarrow R_3 \\
        \begin{pmatrix} 1 & -1.5 & 0.5 \\ 0 & 1 & 0.000004999 \\ 0 & 0 & 1 \end{pmatrix}
    \end{align*}
\end{itemize}

\newpage
\subsection{Least squares and curve fitting}
This problem will give you practice in using least squares to fit polynomials to data. Consider the following dataset in Matlab, consisting of pairs of points $(x, y)$:
\begin{verbatim}
    x = [-3, -2, -1, 0, 1, 2, 3]’;
    y = [-12.529999999999999, -3.02, 0.49, 1.00, 1.51, 5.02, 14.529999999999999]’;
\end{verbatim}
Suppose that you want to find a least squares fit of a degree-3 polynomial to this data. I.e., you want to determine coefficients $c0$, $c1$, $c2$, $c3$ such that $c_3x^3 + c_2x^2 + c1x + c0$ is a least-squares fit to the data. We will collect the coefficients into a vector $\overrightarrow{c_*} = [c0, c1, c2, c3]^T$.
\begin{itemize}
    \item[1.] Using Matlab, compute the Vandermonde matrix A associated with this problem and explain how you got it. Remember that least squares requires us to solve
    \[\overrightarrow{c_*} = \text{argmin}_{\overrightarrow{c}}\left\Vert A\overrightarrow{c}-\overrightarrow{y}\right\Vert^2_2\]
    \newline\newline
    I begin by computing the Vandermonde matrix of x using the format laid out by the textbook as my guideline for its construction. 
    \begin{verbatim}
        disp("The Vandermonde matrix of the problem:");
        A = [ones(size(x)) x x.^2 x.^3];
        disp(A);
    \end{verbatim}
    I run and obtain:
    \begin{verbatim}
        The Vandermonde matrix of the problem:
             1    -3     9   -27
             1    -2     4    -8
             1    -1     1    -1
             1     0     0     0
             1     1     1     1
             1     2     4     8
             1     3     9    27
    \end{verbatim}
    
    \newpage
    \item[2.] In Matlab, use the $A$ that you computed and the normal equations approach to the least squares problem to compute the vector of coefficients $\overrightarrow{c}$.
    \newline\newline
    I compute the vector of coefficients using the pseudo-inverse of $A$ and $y$.
    \begin{verbatim}
        disp("The Vector of coefficients:");
        c = pinv(A)*y;
        disp(c);
    \end{verbatim}
     I run and obtain:
    \begin{verbatim}
        The Vector of coefficients:
            1.0000
            0.0100
            0.0000
            0.5000
    \end{verbatim}
    To check my work I plot the degree 3 polynomial with the computed coefficients against the input data:
    \begin{verbatim}
        hold on
        scatter(x,y)
        fplot(@(x)c(4)*x.^3 + c(3)*x.^2 + c(2)*x + c(1));
        xlim([-5 5])
        ylim([-16 16])
        hold off
    \end{verbatim}
    And observe that this does indeed appear to fit the data very well:
    
    
    \item[3.] In Matlab, use the norm function to compute the the error $\left\Vert A\overrightarrow{c}-\overrightarrow{y}\right\Vert^2_2$.
\end{itemize}

\subsection{Eigenthings and the power method}
\begin{itemize}
    \item[1.] Using Matlab, use the power method to calculate unit eigenvectors corresponding to the dominant two eigenvalues of the matrix
    \[\begin{pmatrix} 6 & 2 & -1 \\ 2 & 5 & 1 \\ -1 & 1 & 4 \end{pmatrix}\]
    In particular, you should run the power method for 50 iterations. As usual, include your Matlab code and output. Clearly indicate the two eigenvectors that are your final answer.
\end{itemize}

\subsection{Polynomial interpolation, Chebyshev points}
\begin{itemize}
    \item[1.] In general, if we are given points $(x_0, y_0), ...,(x_n, y_n)$, what is the minimum number $d$ such that a unique polynomial with degree $\leq d$ and passing through these points is guaranteed to exist?
    \item[2.] Let $x_0 = −1, x_1 = 1, x_2 = 2$, and let $y_0 = 1, y_1 = 2, y_2 = 3$. Write down the Lagrange interpolating polynomial that passes through the points $(x_0, y_0),(x_1, y_1),(x_2, y_2)$
    \item[3.] For the points in the previous problem, fill in the following divided difference table:
    \begin{center}
        \begin{tabular}{ |c|c|c| } 
         \hline
         $f[x_0]$ &  &  \\ 
         $f[x_1]$ & $f[x_0, x_1]$ &  \\ 
         $f[x_2]$ & $f[x_1, x_2]$ & $f[x_0, x_1, x_2]$ \\ 
         \hline
        \end{tabular}
    \end{center}
    In other words, what is expected is that you compute $f[x_0], f[x_1], f[x_0, x_1]$, etc.
    \item[4.] Using the divided difference table above, write down the Newton interpolating polynomial for these points.
\end{itemize}

\subsection{Piecewise polynomial interpolation}
Do Exercise 8.8.9 in the book (on piecewise polynomial interpolation).

\end{document}